---
title: "CT"
author: "Jens Daniel Müller & Lara Burchardt"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  workflowr::wflow_html:
    number_sections: true
    toc_depth: 3
    toc_float:
      collapsed: false
editor_options:
  chunk_output_type: console
---


```{r global_options, include = FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r library, message=FALSE, warning=FALSE}

library(tidyverse)
library(ncdf4)
library(vroom)
library(lubridate)
library(geosphere)
library(dygraphs)
library(xts)
library(here)
library(seacarb)
library(zoo)

```


```{r ggplot_theme, include = FALSE}
theme_set(theme_bw())
```


```{r subsetting_criteria}

# route
select_route <- c("E", "F", "G", "W", "X") 

# variable names in 2d and 3d GETM files
var <- "SSS_east"

# latitude limits
low_lat <- 58.5
high_lat <- 59.0

```

# Regional mean salinity

The mean salinity was calculated across all measurments made between march - september in the NGS subregion. 


```{r read_sss_data_fm_ngs}

nc <- nc_open(paste("data/Finnmaid/", "FM_all_2019_on_standard_tracks.nc", sep = ""))

# read required vectors from netcdf file
route <- ncvar_get(nc, "route")
route <- unlist(strsplit(route, ""))
date_time <- ncvar_get(nc, "time")
latitude_east <- ncvar_get(nc, "latitude_east")

array <- ncvar_get(nc, var) # store the data in a 2-dimensional array
#dim(array) # should have 2 dimensions: 544 coordinate, 2089 time steps
  
  fillvalue <- ncatt_get(nc, var, "_FillValue")
  array[array == fillvalue$value] <- NA
  rm(fillvalue)
  
  #i <- 5
  for (i in seq(1,length(route),1)){
  
      
    if(route[i] %in% select_route) {
      slice <- array[i,]
      
      value <- mean(slice[latitude_east > low_lat & latitude_east < high_lat], na.rm = TRUE)
      sd    <- sd(slice[latitude_east > low_lat & latitude_east < high_lat], na.rm = TRUE)
      date <- ymd("2000-01-01") + date_time[i]
      
      temp <- bind_cols(date = date, var=var, value = value, sd = sd)
      
      if (exists("timeseries", inherits = FALSE)){
        timeseries <- bind_rows(timeseries, temp)
      } else{timeseries <- temp}
      
      rm(temp, value, date, sd)
      
    } 
  }
nc_close(nc)

fm_sss__ngs <- timeseries %>% 
  mutate(sss = value,
         year = year(date),
         month = month(date))

fm_sss_ngs_monthlymean <- fm_sss__ngs %>% 
  filter(month >=3 , month <=9) %>% 
  summarise(sss_mean = mean(sss, na.rm = TRUE))


rm(array,fm_sss__ngs,nc, timeseries, date_time, 
  i, latitude_east, route, slice, var)
```

The mean salinity between March and September for the NGS subregion for all years is `r round(fm_sss_ngs_monthlymean,2)`.

# Finnmaid data extraction

pCO~2~ and SST observations in NGS were extracted for all crossings.

```{r fm_ngs_all_routes}
nc <- nc_open(paste("data/Finnmaid/", "FM_all_2019_on_standard_tracks.nc", sep = ""))

#names(nc$var) # uncomment to print variable names and select relevant index
index <- c(9,11) #index of wanted variables SST_east and pCO2 east
var_all <- c(names(nc$var[index]))

# read required vectors from netcdf file
route <- ncvar_get(nc, "route")
route <- unlist(strsplit(route, ""))
date_time <- ncvar_get(nc, "time")
latitude_east <- ncvar_get(nc, "latitude_east")

for (var in var_all) {
  
  #print(var)
  
  array <- ncvar_get(nc, var) # store the data in a 2-dimensional array
  #dim(array) # should have 2 dimensions: 544 coordinate, 2089 time steps
  
  fillvalue <- ncatt_get(nc, var, "_FillValue")
  array[array == fillvalue$value] <- NA
  rm(fillvalue)
  
    for (i in seq(1,length(route),1)){
  
      
    if(route[i] %in% select_route) {
      slice <- array[i,]
      
      value <- mean(slice[latitude_east > low_lat & latitude_east < high_lat], na.rm = TRUE)
      sd    <- sd(slice[latitude_east > low_lat & latitude_east < high_lat], na.rm = TRUE)
      date <- ymd("2000-01-01") + date_time[i]
      
      fm_ngs_all_routes_part <- bind_cols(date = date, var=var, value = value, sd = sd, route=route[i])
      
      if (exists("fm_ngs_all_routes", inherits = FALSE)){
        fm_ngs_all_routes <- bind_rows(fm_ngs_all_routes, fm_ngs_all_routes_part)
      } else{fm_ngs_all_routes <- fm_ngs_all_routes_part}
      
      rm(fm_ngs_all_routes_part, value, date, sd, slice)
      
      } 
    }
  rm(array, var,i)
}   
      
nc_close(nc)

fm_ngs_all_routes %>%  
  write_csv(here::here("data/_summarized_data_files/", file = "fm_ngs_all_routes.csv"))

rm(nc, fm_ngs_all_routes, latitude_east, route,date_time)

```

# GETM windspeed

Reanalysis windspeed data as used in the GETM model run were used.

```{r read_getm_2d_mld_wind, eval=FALSE}

filesList_2d <- list.files(path= "data", pattern = "Finnmaid.E.2d.20", recursive = TRUE)

file <- filesList_2d[1]
nc <- nc_open(paste("data/", file, sep = ""))

names(nc$var)
index <- c(11,12) # index of wanted variables u10 and v10
var_all <- c(names(nc$var[index]))


lon <- ncvar_get(nc, "lonc")
lat <- ncvar_get(nc, "latc", verbose = F)
nc_close(nc)

rm(file, nc)

for (var in var_all){

for (n in 1:length(filesList_2d)) {

file <- filesList_2d[n]

nc <- nc_open(paste("data/", file, sep = ""))

time_units <- nc$dim$time$units %>%     #we read the time unit from the netcdf file to calibrate the time 
    substr(start = 15, stop = 33) %>%   #calculation, we take the relevant information from the string
    ymd_hms()                           # and transform it to the right format

t <- time_units + ncvar_get(nc, "time")

array <- ncvar_get(nc, var) # store the data in a 2-dimensional array
dim(array) # should be 2d with dimensions: 544 coordinate, 31d*(24h/d/3h)=248 time steps

array <- as.data.frame(t(array), xy=TRUE)
array <- as_tibble(array)

gt_windspeed_ngs_part <- array %>%
  set_names(as.character(lat)) %>%
  mutate(date_time = t) %>%
  gather("lat", "value", 1:length(lat)) %>%
  mutate(lat = as.numeric(lat)) %>%
  filter(lat > low_lat, lat<high_lat) %>%
  group_by(date_time) %>%
  summarise_all("mean") %>%
  ungroup() %>%
  mutate(var = var)


if (exists("gt_windspeed_ngs")) {gt_windspeed_ngs <- bind_rows(gt_windspeed_ngs, gt_windspeed_ngs_part)
  }else {gt_windspeed_ngs <- gt_windspeed_ngs_part}

  nc_close(nc)
  rm(array, nc, t, gt_windspeed_ngs_part)
  print(n) # to see working progress
}

print(paste("gt_", var, "_ngs.csv", sep = "")) # to see working progress


rm(n, file, time_units)
}

rm(filesList_2d, var, var_all, lat, lon)


gt_windspeed_ngs <- gt_windspeed_ngs %>% 
  group_by(date_time, var) %>% 
  summarise(mean_value= mean(value)) %>% 
  pivot_wider(values_from = mean_value, names_from = var) %>% 
  mutate(U_10 = round(sqrt(u10^2 + v10^2), 3)) %>% 
  select(-c(u10, v10))

gt_windspeed_ngs %>% 
  write_csv(here::here("data/_summarized_data_files/", file = paste("gt_windspeed_ngs.csv")))

rm(gt_windspeed_ngs)

```


# C~T~ calculation

C~T~ was calculated from measured pCO~2~ based on a fixed mean alkalinity value of 1650 µmol kg-1.

```{r calculate_CT_from_pCO2_Finnmaid, eval=FALSE}

df <- read_csv(here::here("data/_summarized_data_files/", file =  "fm_ngs_all_routes.csv"))

df <- df %>%
  select(date, var, value, route) 

df <- df %>%
  # drop_na() %>% 
  pivot_wider(values_from = value, names_from = var) %>% 
  drop_na()

#calculation of CT based on pCO2 (var1) and alkalinity (var2) as input parameters
#calculation of CT in theoretical equilibrium with atmosphere (CT_equi) based on pCO2_air (var1) and alkalinity (var2) as input parameters

df <- df %>%
  rename(SST = SST_east,
         pCO2 = pCO2_east) %>% 
  #drop_na() %>% 
  mutate(CT = carb(24,
                   var1=pCO2,
                   var2=1650*1e-6,
                   S=pull(fm_sss_ngs_monthlymean),
                   T=SST,
                   k1k2="m10", kf="dg", ks="d", gas="insitu")[,16]*1e6) %>% 
  mutate(year = year(date),
         pCO2_air = 400 - 2*(2015-year),
         CT_equi = carb(24,
                   var1=pCO2_air,
                   var2=1650*1e-6,
                   S=pull(fm_sss_ngs_monthlymean),
                   T=SST,
                   k1k2="m10", kf="dg", ks="d", gas="insitu")[,16]*1e6) %>% 
  select(-c(pCO2_air, year))


df %>% 
   write_csv(here::here("data/_summarized_data_files/", file = "fm_CT_ngs.csv"))

rm(df)
```

# Air-sea CO~2~ flux

The CO~2~ flux across the sea surface was calculated according to Wanninkhof (2014).


```{r calculate_gas_exchange_from_Finnmaid_pCO2_S_T_and_GETM_wind, eval=FALSE}

df_1 <- read_csv(here::here("data/_summarized_data_files/", file = "fm_CT_ngs.csv"))
df_2 <- read_csv(here::here("data/_summarized_data_files/", file = "gt_windspeed_ngs.csv"))

df_2 <- df_2 %>% 
  mutate(date = as.Date(date_time)) %>% 
  select(date, U_10) %>% 
  group_by(date) %>% 
  summarise_all("mean") %>% 
  ungroup()

df <- full_join(df_1, df_2, by = "date") %>% 
  arrange(date)

rm(df_1,df_2)

df <- df %>% 
  mutate(year = year(date),
         pCO2_int = na.approx(pCO2, na.rm = FALSE),                  #na.approx: replacing NA with interpolated values
         SST_int = na.approx(SST, na.rm = FALSE)) %>% 
  filter(!is.na(pCO2_int))

#Calculation of the Schmidt number as a funktion of temperature according to Wanninkhof (2014)
Sc_W14 <- function(tem) {
  2116.8 - 136.25 * tem + 4.7353 * tem^2 - 0.092307 * tem^3 + 0.0007555 * tem^4
}

Sc_W14(20)

# calculate flux F [mol m–2 d–1]
df <- df %>%
  mutate(pCO2_air = 400 - 2*(2015-year),
         dpCO2 = pCO2_int - pCO2_air,
         dCO2  = dpCO2 * K0(S=pull(fm_sss_ngs_monthlymean), T=SST_int),
         k     = 0.251 * U_10^2 * (Sc_W14(SST_int)/660)^(-0.5),
         flux_daily = k*dCO2*1e-5*24)


df %>% 
   write_csv(here::here("data/_merged_data_files/", file = paste("gt_fm_flux_ngs.csv")))

rm(df, Sc_W14)
```

# Time series C~T~, MLD, SST, windspeed, and air-sea fluxes

```{r visualization_CT_flux_windspeeds, fig.asp=0.5}

# read CT and flux data
gt_fm_flux_ngs <- read_csv(here::here("data/_merged_data_files/", file = "gt_fm_flux_ngs.csv"))

gt_fm_flux_ngs <- gt_fm_flux_ngs %>% 
  mutate(date = as.Date(date))

ts_xts_CT <- xts(cbind(gt_fm_flux_ngs$CT, gt_fm_flux_ngs$CT_equi), order.by = gt_fm_flux_ngs$date)
names(ts_xts_CT) <- c("CT", "CT_equi")

ts_xts_SST <- xts(gt_fm_flux_ngs$SST, order.by = gt_fm_flux_ngs$date)
names(ts_xts_SST) <- "SST"

ts_xts_windspeed <- xts(gt_fm_flux_ngs$U_10, order.by = gt_fm_flux_ngs$date)
names(ts_xts_windspeed) <- "Windspeed"

ts_xts_flux <- xts(gt_fm_flux_ngs$flux_daily, order.by = gt_fm_flux_ngs$date)
names(ts_xts_flux) <- "Daily Flux"


# read MLD data
gt_mld_fm_pco2_ngs <-
  read_csv(here::here("data/_merged_data_files/", file = "gt_mld_fm_pco2_ngs.csv"))

gt_mld_fm_pco2_ngs <- gt_mld_fm_pco2_ngs %>% 
  mutate(date = as.Date(date))

ts_xts_mld5 <- xts(gt_mld_fm_pco2_ngs$value_mld5, order.by = gt_mld_fm_pco2_ngs$date)
names(ts_xts_mld5) <- "mld_age_5"

ts_xts_CT %>% 
  dygraph(group = "Fluxes") %>% 
  dyRangeSelector(dateWindow = c("2014-01-01", "2016-12-31")) %>% 
  dySeries("CT") %>% 
  dyAxis("y", label = "CT") %>% 
  dyOptions(drawPoints = TRUE, pointSize = 1.5, connectSeparatedPoints=TRUE, strokeWidth=0.5,
            drawAxesAtZero=TRUE)

ts_xts_SST %>% 
  dygraph(group = "Fluxes") %>% 
  dyRangeSelector(dateWindow = c("2014-01-01", "2016-12-31")) %>% 
  dySeries("SST") %>% 
  dyAxis("y", label = "SST") %>% 
  dyOptions(drawPoints = TRUE, pointSize = 1.5, connectSeparatedPoints=TRUE, strokeWidth=0.5,
            drawAxesAtZero=TRUE)

ts_xts_mld5 %>% 
  dygraph(group = "Fluxes") %>% 
  dyRangeSelector(dateWindow = c("2014-01-01", "2016-12-31")) %>% 
  dySeries("mld_age_5") %>% 
  dyAxis("y", label = "mld_age_5") %>% 
  dyOptions(drawPoints = TRUE, pointSize = 1.5, connectSeparatedPoints=TRUE, strokeWidth=0.5,
            drawAxesAtZero=TRUE)

ts_xts_windspeed %>% 
  dygraph(group = "Fluxes") %>% 
  dyRangeSelector(dateWindow = c("2014-01-01", "2016-12-31")) %>% 
  dySeries("Windspeed") %>% 
  dyAxis("y", label = "Windspeed [m/s]") %>% 
  dyOptions(drawPoints = TRUE, pointSize = 1.5, connectSeparatedPoints=TRUE, strokeWidth=0.5,
            drawAxesAtZero=TRUE)

ts_xts_flux %>% 
  dygraph(group = "Fluxes") %>% 
  dyRangeSelector(dateWindow = c("2014-01-01", "2016-12-31")) %>% 
  dySeries("Daily Flux") %>% 
  dyAxis("y", label = "Daily Flux") %>% 
  dyOptions(drawPoints = TRUE, pointSize = 1.5, connectSeparatedPoints=TRUE, strokeWidth=0.5)

rm(gt_fm_flux_ngs, gt_mld_fm_pco2_ngs,
   ts_xts_CT, ts_xts_flux, ts_xts_mld5, ts_xts_SST, ts_xts_windspeed)
```

# Identification of continous deployment periods

```{r deployment_criteria}

# time

time_low <- 03 # 03 as month March
time_high <- 09 # 09 as month September

deployment_gap <- 7

```

- The maximum allowed gap was defined as `r deployment_gap` day.

```{r deployments}

df <- 
   read_csv(here::here("data/_summarized_data_files/", file = "fm_CT_ngs.csv"))

df <- df %>% 
  mutate (month = month(date),
          year = year(date)) %>% 
  filter (month >= time_low & month <= time_high) %>% 
  group_by(year) %>% 
  mutate(deployment = 
           as.factor(cumsum(c(TRUE,diff(date)>= deployment_gap)))) %>% # deployment +1, when data gap > 7 days
  ungroup()

df %>% 
  ggplot(aes( x = as.Date(yday(date)), y = year, color = deployment))+
  geom_point()+
  scale_y_reverse(breaks = seq(2000,2030,1))+
  scale_x_date(date_minor_breaks = "week",
               date_labels = "%b")+
  scale_color_brewer(palette = "Set1", name="PPP")+
  theme(axis.title.x = element_blank())


df %>% 
  write_csv(here::here("data/_summarized_data_files/", file="fm_CT_ngs_deployments.csv"))

```


# Identification of primary production periods (PPP)

```{r PPP_criteria}

#criteria
decrease_start <- 20
timespan_start <- 7 #in days

decrease_end <- 20
timespan_end <- 7 #in days

```


The following criteria are implemented in the following to find periods of primary production:

- CT is lower than at equilibrium with the atmosphere
- period must be within one deployment
- starts at a day followed by `r timespan_start` days in which CT decreased at least by `r decrease_start` 
- ends at day where the CT drop was below `r decrease_end` for the previous `r timespan_end`
 
PPPs are numbered. 
 
```{r primary_production_periods}

# databasis

df <- 
   read_csv(here::here("data/_summarized_data_files/", file = "fm_CT_ngs_deployments.csv"))

# identification

#ppp identified per year, per deployment, only when CT < CT_equi


df_ppp <- df %>% 
  filter(CT < CT_equi)

years <- (unique(df_ppp$year))

# first of three loops, loops through each observatio year
# we get the number of deployments within this year for further analysis
for (n in years) {
  
  deployment <- df_ppp %>% 
    filter(year == n)
    deployment <- max(deployment$deployment)
  
  a <- 1 #ppp enumeration starts new for each year
  start <- "no" #start variable is set "no" for condition to work, no start possible at beginning of a new deployment
  
  # second of three loops, loops through each deployment within on year
  for (d in 1:deployment){
   
    df_ppp_temp <- df_ppp %>% 
      filter(year == n  , deployment == d) %>% 
      mutate(ppp = NA)
    
    
    # third of three loops, within on deployment of year n, we check ppp criteria for every row
    for (x in 1:nrow(df_ppp_temp)){
  
    if (start == "start"){a <- a 
    } else {a <- a + 1} #while we are in a ppp (checked by start == start) the control number stays the same, if start was no, it is counted up, for every loop it is "no" variable gets counted up, because of that, reassignment at the end still necessary, but reliable, becuase not with date gap, but with ppp gap
  
    ##start criteria
    # define subdataset looking forward "timespan_start" days
    lag_start <- df_ppp_temp %>% 
    filter(date > date[x], date <= (date[x]+ duration(timespan_start, 'days')))
    
    
    #roll_index <- which.max(lag_start$date) 
    roll_index <- which.min(lag_start$CT)  #Minimum CT-value in 7 day forward
    roll_value <- lag_start$CT[roll_index] # get exact minimum CT value within subdataset 
    
    # we only proceed, if there is a minimum CT value
    if (is_empty(roll_value)== FALSE){
    
    # get index of exact CT value in whole dataset 
    roll_index_df <- df_ppp_temp %>%
    mutate(row_no = row_number()) %>%
    filter(CT == roll_value) %>%
    select(row_no) %>% 
    as.numeric()
    
    # second if-condition for start criteria; is CT value of current loop date x more than "decrease_start" higher tha minimum CT in 7 day forward
    
    if (df_ppp_temp$CT[x]-roll_value >= decrease_start) {
    
    start <- "start" #condition for end-criteria; only TRUE when there was the necessary decrease
    
    #new ppp value is only assigned, if ppp currently is NA, that way mixing of numbers because of jumps in data is prohibited
    
   if(is.na(df_ppp_temp$ppp[x:roll_index_df])){
    df_ppp_temp$ppp[x:roll_index_df] = as.numeric(a) #loop date x : minimum CT within 7 days forward get assigned the same ppp
    
   } else{}
    
    } else{df_ppp_temp$ppp[x] = NA
           start <- "no"}
    rm(roll_index_df)
    } else {start <- "no"} #if we don't have a roll_value or the criterion of decrease is not met start is set FALSE, so that end criteria don't kick in without a start
    
    ##end criteria
    
    #was there a start? then same procdure as for start criterion but looking backwards
    if (start == "start"){
    
    lag_end <- df_ppp_temp %>% 
    filter(date < date[x], date >= (date[x]- duration(timespan_end, 'days')))

    #roll_index <- which.max(lag_end$date) 
    roll_index <- which.max(lag_end$CT) #in contrast to start we search for the maximum CT value here when looking backwards
    roll_value <- lag_end$CT[roll_index]
    
    if (is_empty(roll_value) == FALSE){ #at the start of the loop we can't look backwards
      
    roll_index_df <- df_ppp_temp %>%
    mutate(row_no = row_number()) %>%
    filter(CT == roll_value) %>%
    select(row_no) %>% 
    as.numeric()
  
    if (df_ppp_temp$CT[x]-roll_value >= decrease_end){
    
    df_ppp_temp$ppp[x:roll_index_df] = as.numeric(a) #loop date x : minimum CT within 7 days backwards get assigned the same ppp, if criterion was met
 rm(roll_index_df)
    } else{} #if criterion is not met, we do nothing as NAs are already assigned in start and this would potentially overwrite an existing ppp entry
    
    } else {next} #if we can't look backwards, we jump to the next loop iteration
    
    if (exists("fm_ppp", inherits = FALSE)){
        fm_ppp <- bind_rows(fm_ppp, df_ppp_temp)
        }else{fm_ppp <- df_ppp_temp}
    
    rm(lag_end)
    
      }#end start == TRUE condition
    
    rm(roll_index, roll_value, lag_start)
    
    } #end  loop through df_ppp_temp
  }#end of loop through deployments
}#end of loop through years

# remove duplicates and NAs
fm_ppp_dup <- fm_ppp[!duplicated(fm_ppp),] %>% 
  arrange(date)

fm_ppp_dup_na <- fm_ppp_dup %>% 
  drop_na()

# enumeration of ppps, remove duplicates again 

#ppps are now reliably numbered, but not correctly from 1-2-3 etc. so they get renumbered again, new number starting from 1 whenever the gap in ppp is > 1
fm_ppp_final <- fm_ppp_dup_na %>% 
  group_by(year) %>%
  mutate(ppp = as.factor(cumsum(c(TRUE,diff(ppp)> 1)))) %>% 
  ungroup()

fm_ppp_final <- fm_ppp_final[!duplicated(fm_ppp_final),]

ggplot()+
  geom_point(data = df, aes(as.Date(yday(date)), year), col="grey")+
  geom_point(data = fm_ppp_final, aes(as.Date(yday(date)), year, col = as.factor(ppp)))+
  scale_y_reverse(breaks = seq(2000,2030,1))+
  scale_x_date(date_minor_breaks = "week",
               date_labels = "%b")+
  scale_color_brewer(palette = "Set1", name="PPP")+
  theme(axis.title.x = element_blank())

fm_ppp_final %>% 
  write_csv(here::here("data/_summarized_data_files/", file = "fm_ppp_ngs.csv"))


```


## CT yearly time series

```{r CT_timeseries_yearly_PPP, fig.asp=4}

ggplot()+
  geom_point(data = df, aes(as.Date(yday(date)), CT), color = "grey")+
  geom_point(data = fm_ppp_final, aes(as.Date(yday(date)), CT, color = as.factor(ppp)))+
  scale_y_continuous(breaks = seq(1000,2000,100),
                     minor_breaks = seq(1000,2000,20))+
  scale_x_date(date_minor_breaks = "week",
               date_labels = "%b")+
  scale_color_brewer(palette = "Set1", name="PPP")+
  theme(axis.title.x = element_blank(),
        legend.position = "bottom")+
  facet_grid(year~.)

```

## SST yearly time series

```{r SST_timeseries_yearly_PPP, fig.asp=4}

ggplot()+
  geom_point(data = df, aes(as.Date(yday(date)), SST), color = "grey")+
  geom_point(data = fm_ppp_final, aes(as.Date(yday(date)), SST, color = as.factor(ppp)))+
  scale_x_date(date_minor_breaks = "week",
               date_labels = "%b")+
  scale_color_brewer(palette = "Set1", name="PPP")+
  theme(axis.title.x = element_blank(),
        legend.position = "bottom")+
  facet_grid(year~.)

```



# Tasks / open questions

- check drop_na() before CT calculation, because thise removes quite a lot data 
  points where only SST is missing.
  - drop_na() before pivot_wider: leaves 2016 observations
  - drop_na() after pivot_wider or second drop_na() just before CT calculations: leaves 1982 observations, we loose timeperiods without SST value from: "2005-8-25"       to "2005-9-24" and "2006-01-29" to "2006-03-04"
  - still we need to keep drop_na() after pivot_wider for now, because CT calculation does not accept NA data in any column
  
