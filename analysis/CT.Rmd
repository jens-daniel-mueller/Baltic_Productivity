---
title: "CT"
author: "Jens Daniel Müller & Lara Burchardt"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  workflowr::wflow_html:
    number_sections: true
    toc_depth: 3
    toc_float:
      collapsed: false
editor_options:
  chunk_output_type: console
---


```{r global_options, include = FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

```{r library, message=FALSE, warning=FALSE}

library(tidyverse)
library(ncdf4)
library(vroom)
library(lubridate)
library(geosphere)
library(dygraphs)
library(xts)
library(here)
library(seacarb)
library(zoo)
library(RColorBrewer)

```


```{r ggplot_theme, include = FALSE}
theme_set(theme_bw())
```


```{r subsetting_criteria}

# route
select_route <- c("E", "F", "G", "W", "X") 

# variable names in 2d and 3d GETM files
var <- "SSS_east"

# latitude limits
low_lat <- 58.5
high_lat <- 59.0

```

# Regional mean salinity

The mean salinity was calculated across all measurments made between march - september in the NGS subregion. 


```{r read_sss_data_fm_ngs}

nc <- nc_open(paste("data/Finnmaid/", "FM_all_2019_on_standard_tracks.nc", sep = ""))

# read required vectors from netcdf file
route <- ncvar_get(nc, "route")
route <- unlist(strsplit(route, ""))
date_time <- ncvar_get(nc, "time")
latitude_east <- ncvar_get(nc, "latitude_east")

array <- ncvar_get(nc, var) # store the data in a 2-dimensional array
#dim(array) # should have 2 dimensions: 544 coordinate, 2089 time steps
  
  fillvalue <- ncatt_get(nc, var, "_FillValue")
  array[array == fillvalue$value] <- NA
  rm(fillvalue)
  
  #i <- 5
  for (i in seq(1,length(route),1)){
  
      
    if(route[i] %in% select_route) {
      slice <- array[i,]
      
      value <- mean(slice[latitude_east > low_lat & latitude_east < high_lat], na.rm = TRUE)
      sd    <- sd(slice[latitude_east > low_lat & latitude_east < high_lat], na.rm = TRUE)
      date <- ymd("2000-01-01") + date_time[i]
      
      temp <- bind_cols(date = date, var=var, value = value, sd = sd)
      
      if (exists("timeseries", inherits = FALSE)){
        timeseries <- bind_rows(timeseries, temp)
      } else{timeseries <- temp}
      
      rm(temp, value, date, sd)
      
    } 
  }
nc_close(nc)

fm_sss__ngs <- timeseries %>% 
  mutate(sss = value,
         year = year(date),
         month = month(date))

fm_sss_ngs_monthlymean <- fm_sss__ngs %>% 
  filter(month >=3 , month <=9) %>% 
  summarise(sss_mean = mean(sss, na.rm = TRUE))


rm(array,fm_sss__ngs,nc, timeseries, date_time, 
  i, latitude_east, route, slice, var)
```

The mean salinity between March and September for the NGS subregion for all years is `r round(fm_sss_ngs_monthlymean,2)`.

# Finnmaid data extraction

pCO~2~ and SST observations in NGS were extracted for all crossings.

```{r fm_ngs_all_routes}
nc <- nc_open(paste("data/Finnmaid/", "FM_all_2019_on_standard_tracks.nc", sep = ""))

#names(nc$var) # uncomment to print variable names and select relevant index
index <- c(9,11) #index of wanted variables SST_east and pCO2 east
var_all <- c(names(nc$var[index]))

# read required vectors from netcdf file
route <- ncvar_get(nc, "route")
route <- unlist(strsplit(route, ""))
date_time <- ncvar_get(nc, "time")
latitude_east <- ncvar_get(nc, "latitude_east")

for (var in var_all) {
  
  #print(var)
  
  array <- ncvar_get(nc, var) # store the data in a 2-dimensional array
  #dim(array) # should have 2 dimensions: 544 coordinate, 2089 time steps
  
  fillvalue <- ncatt_get(nc, var, "_FillValue")
  array[array == fillvalue$value] <- NA
  rm(fillvalue)
  
    for (i in seq(1,length(route),1)){
  
      
    if(route[i] %in% select_route) {
      slice <- array[i,]
      
      value <- mean(slice[latitude_east > low_lat & latitude_east < high_lat], na.rm = TRUE)
      sd    <- sd(slice[latitude_east > low_lat & latitude_east < high_lat], na.rm = TRUE)
      date <- ymd("2000-01-01") + date_time[i]
      
      fm_ngs_all_routes_part <- bind_cols(date = date, var=var, value = value, sd = sd, route=route[i])
      
      if (exists("fm_ngs_all_routes", inherits = FALSE)){
        fm_ngs_all_routes <- bind_rows(fm_ngs_all_routes, fm_ngs_all_routes_part)
      } else{fm_ngs_all_routes <- fm_ngs_all_routes_part}
      
      rm(fm_ngs_all_routes_part, value, date, sd, slice)
      
      } 
    }
  rm(array, var,i)
}   
      
nc_close(nc)

fm_ngs_all_routes %>%  
  write_csv(here::here("data/_summarized_data_files/", file = "fm_ngs_all_routes.csv"))

rm(nc, fm_ngs_all_routes, latitude_east, route,date_time)

```

# GETM windspeed

Reanalysis windspeed data as used in the GETM model run were used.

```{r read_getm_2d_mld_wind, eval=FALSE}

filesList_2d <- list.files(path= "data", pattern = "Finnmaid.E.2d.20", recursive = TRUE)

file <- filesList_2d[1]
nc <- nc_open(paste("data/", file, sep = ""))

names(nc$var)
index <- c(11,12) # index of wanted variables u10 and v10
var_all <- c(names(nc$var[index]))


lon <- ncvar_get(nc, "lonc")
lat <- ncvar_get(nc, "latc", verbose = F)
nc_close(nc)

rm(file, nc)

for (var in var_all){

for (n in 1:length(filesList_2d)) {

file <- filesList_2d[n]

nc <- nc_open(paste("data/", file, sep = ""))

time_units <- nc$dim$time$units %>%     #we read the time unit from the netcdf file to calibrate the time 
    substr(start = 15, stop = 33) %>%   #calculation, we take the relevant information from the string
    ymd_hms()                           # and transform it to the right format

t <- time_units + ncvar_get(nc, "time")

array <- ncvar_get(nc, var) # store the data in a 2-dimensional array
dim(array) # should be 2d with dimensions: 544 coordinate, 31d*(24h/d/3h)=248 time steps

array <- as.data.frame(t(array), xy=TRUE)
array <- as_tibble(array)

gt_windspeed_ngs_part <- array %>%
  set_names(as.character(lat)) %>%
  mutate(date_time = t) %>%
  gather("lat", "value", 1:length(lat)) %>%
  mutate(lat = as.numeric(lat)) %>%
  filter(lat > low_lat, lat<high_lat) %>%
  group_by(date_time) %>%
  summarise_all("mean") %>%
  ungroup() %>%
  mutate(var = var)


if (exists("gt_windspeed_ngs")) {gt_windspeed_ngs <- bind_rows(gt_windspeed_ngs, gt_windspeed_ngs_part)
  }else {gt_windspeed_ngs <- gt_windspeed_ngs_part}

  nc_close(nc)
  rm(array, nc, t, gt_windspeed_ngs_part)
  print(n) # to see working progress
}

print(paste("gt_", var, "_ngs.csv", sep = "")) # to see working progress


rm(n, file, time_units)
}

rm(filesList_2d, var, var_all, lat, lon)


gt_windspeed_ngs <- gt_windspeed_ngs %>% 
  group_by(date_time, var) %>% 
  summarise(mean_value= mean(value)) %>% 
  pivot_wider(values_from = mean_value, names_from = var) %>% 
  mutate(U_10 = round(sqrt(u10^2 + v10^2), 3)) %>% 
  select(-c(u10, v10))

gt_windspeed_ngs %>% 
  write_csv(here::here("data/_summarized_data_files/", file = paste("gt_windspeed_ngs.csv")))

rm(gt_windspeed_ngs)

```


# C~T~ calculation

C~T~ was calculated from measured pCO~2~ based on a fixed mean alkalinity value of 1650 µmol kg-1.

```{r calculate_CT_from_pCO2_Finnmaid, eval=FALSE}

df <- read_csv(here::here("data/_summarized_data_files/", file =  "fm_ngs_all_routes.csv"))

df <- df %>%
  select(date, var, value, route) 

df <- df %>%
  # drop_na() %>% 
  pivot_wider(values_from = value, names_from = var) %>% 
  drop_na()

#calculation of CT based on pCO2 (var1) and alkalinity (var2) as input parameters
#calculation of CT in theoretical equilibrium with atmosphere (CT_equi) based on pCO2_air (var1) and alkalinity (var2) as input parameters

df <- df %>%
  rename(SST = SST_east,
         pCO2 = pCO2_east) %>% 
  #drop_na() %>% 
  mutate(CT = carb(24,
                   var1=pCO2,
                   var2=1650*1e-6,
                   S=pull(fm_sss_ngs_monthlymean),
                   T=SST,
                   k1k2="m10", kf="dg", ks="d", gas="insitu")[,16]*1e6) %>% 
  mutate(year = year(date),
         pCO2_air = 400 - 2*(2015-year),
         CT_equi = carb(24,
                   var1=pCO2_air,
                   var2=1650*1e-6,
                   S=pull(fm_sss_ngs_monthlymean),
                   T=SST,
                   k1k2="m10", kf="dg", ks="d", gas="insitu")[,16]*1e6) %>% 
  select(-c(pCO2_air, year))


df %>% 
   write_csv(here::here("data/_summarized_data_files/", file = "fm_CT_ngs.csv"))

rm(df)
```

# Air-sea CO~2~ flux

The CO~2~ flux across the sea surface was calculated according to Wanninkhof (2014).


```{r calculate_gas_exchange_from_Finnmaid_pCO2_S_T_and_GETM_wind, eval=FALSE}

df_1 <- read_csv(here::here("data/_summarized_data_files/", file = "fm_CT_ngs.csv"))
df_2 <- read_csv(here::here("data/_summarized_data_files/", file = "gt_windspeed_ngs.csv"))

df_2 <- df_2 %>% 
  mutate(date = as.Date(date_time)) %>% 
  select(date, U_10) %>% 
  group_by(date) %>% 
  summarise_all("mean") %>% 
  ungroup()

df <- full_join(df_1, df_2, by = "date") %>% 
  arrange(date)

rm(df_1,df_2)

df <- df %>% 
  mutate(year = year(date),
         pCO2_int = na.approx(pCO2, na.rm = FALSE),                  #na.approx: replacing NA with interpolated values
         SST_int = na.approx(SST, na.rm = FALSE)) %>% 
  filter(!is.na(pCO2_int))

#Calculation of the Schmidt number as a funktion of temperature according to Wanninkhof (2014)
Sc_W14 <- function(tem) {
  2116.8 - 136.25 * tem + 4.7353 * tem^2 - 0.092307 * tem^3 + 0.0007555 * tem^4
}

Sc_W14(20)

# calculate flux F [mol m–2 d–1]
df <- df %>%
  mutate(pCO2_air = 400 - 2*(2015-year),
         dpCO2 = pCO2_int - pCO2_air,
         dCO2  = dpCO2 * K0(S=pull(fm_sss_ngs_monthlymean), T=SST_int),
         k     = 0.251 * U_10^2 * (Sc_W14(SST_int)/660)^(-0.5),
         flux_daily = k*dCO2*1e-5*24)


df %>% 
   write_csv(here::here("data/_merged_data_files/", file = paste("gt_fm_flux_ngs.csv")))

rm(df, Sc_W14)
```

# Time series C~T~, MLD, SST, windspeed, and air-sea fluxes

```{r visualization_CT_flux_windspeeds, fig.asp=0.5}

# read CT and flux data
gt_fm_flux_ngs <- read_csv(here::here("data/_merged_data_files/", file = "gt_fm_flux_ngs.csv"))

gt_fm_flux_ngs <- gt_fm_flux_ngs %>% 
  mutate(date = as.Date(date))

ts_xts_CT <- xts(cbind(gt_fm_flux_ngs$CT, gt_fm_flux_ngs$CT_equi), order.by = gt_fm_flux_ngs$date)
names(ts_xts_CT) <- c("CT", "CT_equi")

ts_xts_SST <- xts(gt_fm_flux_ngs$SST, order.by = gt_fm_flux_ngs$date)
names(ts_xts_SST) <- "SST"

ts_xts_windspeed <- xts(gt_fm_flux_ngs$U_10, order.by = gt_fm_flux_ngs$date)
names(ts_xts_windspeed) <- "Windspeed"

ts_xts_flux <- xts(gt_fm_flux_ngs$flux_daily, order.by = gt_fm_flux_ngs$date)
names(ts_xts_flux) <- "Daily Flux"


# read MLD data
gt_mld_fm_pco2_ngs <-
  read_csv(here::here("data/_merged_data_files/", file = "gt_mld_fm_pco2_ngs.csv"))

gt_mld_fm_pco2_ngs <- gt_mld_fm_pco2_ngs %>% 
  mutate(date = as.Date(date))

ts_xts_mld5 <- xts(gt_mld_fm_pco2_ngs$value_mld5, order.by = gt_mld_fm_pco2_ngs$date)
names(ts_xts_mld5) <- "mld_age_5"

ts_xts_CT %>% 
  dygraph(group = "Fluxes") %>% 
  dyRangeSelector(dateWindow = c("2014-01-01", "2016-12-31")) %>% 
  dySeries("CT") %>% 
  dyAxis("y", label = "CT") %>% 
  dyOptions(drawPoints = TRUE, pointSize = 1.5, connectSeparatedPoints=TRUE, strokeWidth=0.5,
            drawAxesAtZero=TRUE)

ts_xts_SST %>% 
  dygraph(group = "Fluxes") %>% 
  dyRangeSelector(dateWindow = c("2014-01-01", "2016-12-31")) %>% 
  dySeries("SST") %>% 
  dyAxis("y", label = "SST") %>% 
  dyOptions(drawPoints = TRUE, pointSize = 1.5, connectSeparatedPoints=TRUE, strokeWidth=0.5,
            drawAxesAtZero=TRUE)

ts_xts_mld5 %>% 
  dygraph(group = "Fluxes") %>% 
  dyRangeSelector(dateWindow = c("2014-01-01", "2016-12-31")) %>% 
  dySeries("mld_age_5") %>% 
  dyAxis("y", label = "mld_age_5") %>% 
  dyOptions(drawPoints = TRUE, pointSize = 1.5, connectSeparatedPoints=TRUE, strokeWidth=0.5,
            drawAxesAtZero=TRUE)

ts_xts_windspeed %>% 
  dygraph(group = "Fluxes") %>% 
  dyRangeSelector(dateWindow = c("2014-01-01", "2016-12-31")) %>% 
  dySeries("Windspeed") %>% 
  dyAxis("y", label = "Windspeed [m/s]") %>% 
  dyOptions(drawPoints = TRUE, pointSize = 1.5, connectSeparatedPoints=TRUE, strokeWidth=0.5,
            drawAxesAtZero=TRUE)

ts_xts_flux %>% 
  dygraph(group = "Fluxes") %>% 
  dyRangeSelector(dateWindow = c("2014-01-01", "2016-12-31")) %>% 
  dySeries("Daily Flux") %>% 
  dyAxis("y", label = "Daily Flux") %>% 
  dyOptions(drawPoints = TRUE, pointSize = 1.5, connectSeparatedPoints=TRUE, strokeWidth=0.5)

rm(gt_fm_flux_ngs, gt_mld_fm_pco2_ngs,
   ts_xts_CT, ts_xts_flux, ts_xts_mld5, ts_xts_SST, ts_xts_windspeed)
```

# Identify continous periods without data gaps

```{r subsetting_criteria_II}

# time

time_low <- 03 # 03 as month March
time_high <- 09 # 09 as month September

deployment_gap <- 7

```


```{r deployments}

df <- 
   read_csv(here::here("data/_summarized_data_files/", file = "fm_CT_ngs.csv"))

df <- df %>% 
  mutate (month = month(date),
          year = year(date)) %>% 
  filter (month >= time_low & month <= time_high) %>% 
  group_by(year) %>% 
  mutate(deployment = 
           as.factor(cumsum(c(TRUE,diff(date)>= deployment_gap)))) %>% # deployment +1, when data gap > 7 days
  ungroup()

df %>% 
  ggplot(aes( x = as.Date(yday(date)), y = year, color = deployment))+
  geom_point()+
  scale_y_reverse(breaks = seq(2000,2030,1))+
  theme(axis.title.x = element_blank())


df %>% 
  write_csv(here::here("data/_summarized_data_files/", file="fm_CT_ngs_deployments.csv"))

```


# Identifying primary productio periods (PPP)

The following criteria are implemented in the following to find periods of primary production:

- CT is lower than at equilibrium with the atmosphere
- period must be within one deployment
- starts at a day followed by  week in which CT decreased at least by 20 
- ends at day where this is not true for the next week
 
 PPPs are numbered. 
 
```{r primary_production_periods, fig.asp=1.5}

#criteria
decrease_start <- 20
timespan_start <- 7 #in days

decrease_end <- 20
timespan_end <- 7 #in days

# databasis

df <- 
   read_csv(here::here("data/_summarized_data_files/", file = "fm_CT_ngs_deployments.csv"))

# identification

#ppp identified per year, per deployment, only when CT < CT_equi

df_ppp <- df %>% 
  filter(CT < CT_equi)

years <- (unique(df_ppp$year))

for (n in years){
  
  deployment <- df_ppp %>% 
    filter(year == n)
  deployment <- max(deployment$deployment)
  
  for (d in 1:deployment){
   
df_ppp_temp <- df_ppp %>% 
  filter(year == n  , deployment == d) %>% 
  mutate(ppp = NA)

    for (x in 1:nrow(df_ppp_temp)){
  
    ##start criteria
    lag_start <- df_ppp_temp %>% 
    filter(date >= date[x], date <= (date[x]+ duration(timespan_start, 'days')))

    
    roll_index <- which.max(lag_start$date) 
    roll_value <- lag_start$CT[roll_index]
    
    if (is_empty(roll_value)== FALSE){
    roll_index_df <- grep(roll_value, df_ppp_temp)
  
    if (df_ppp_temp$CT[x]-roll_value >= decrease_start){
    
    df_ppp_temp$ppp[x:roll_index_df] = as.numeric(x)
 
    } else{}
    } else {}
    ##end criteria
    
    lag_end <- df_ppp_temp %>% 
    filter(date <= date[x], date >= (date[x]- duration(timespan_end, 'days')))

    roll_index <- which.max(lag_end$date) 
    roll_value <- lag_end$CT[roll_index]
    
    if (is_empty(roll_value) == FALSE){
      
    roll_index_df <- grep(roll_value, df_ppp_temp)
  
    if (df_ppp_temp$CT[x]-roll_value >= decrease_end){
    
    df_ppp_temp$ppp[x:roll_index_df] = as.numeric(x)
 
    } else{}
    }else {next}
    
    if (exists("fm_ppp", inherits = FALSE)){
        fm_ppp <- bind_rows(fm_ppp, df_ppp_temp)
    } else{fm_ppp <- df_ppp_temp}
    
     # print(paste("year:", n,
     #          " *** deployment:", d,
     #          " *** day:", x,
     #          " *** ppp:",df_ppp_temp$ppp[x]))
    
    } #end  loop through df_ppp_temp
  }#end of loop through deployments
}#end of loop through years

fm_ppp_dup <- fm_ppp[!duplicated(fm_ppp),] %>% 
  arrange(date)

fm_ppp_dup_na <- fm_ppp_dup %>% 
  drop_na()

ggplot()+
  geom_point(data = fm_ppp_dup, aes(as.Date(yday(date)), year, color = as.factor(ppp)))+
  scale_y_reverse(breaks = seq(2000,2030,1))+
  theme(axis.title.x = element_blank(),
        legend.position = "bottom")


```


# CT during PPP

```{r CT_timeseries_yearly_PPP, fig.asp=3}


ggplot()+
  geom_point(data = fm_ppp_dup, aes(as.Date(yday(date)), CT, color = as.factor(ppp)))+
  theme(axis.title.x = element_blank(),
        legend.position = "bottom")+
  facet_grid(year~.)


```



# Tasks / open questions

- check drop_na() before CT calculation, because thise removes quite a lot data 
  points where only SST is missing.
  - drop_na() before pivot_wider: leaves 2016 observations
  - drop_na() after pivot_wider or second drop_na() just before CT calculations:            leaves 1982 observations, we loose timeperiods without SST value from: "2005-8-25"       to "2005-9-24" and "2006-01-29" to "2006-03-04"
  - still we need to keep drop_na() after pivot_wider for now, because CT calculation does not accept NA data in any column
  
